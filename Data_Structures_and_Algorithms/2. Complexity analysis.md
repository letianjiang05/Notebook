# 2.1   Algorithm efficiency assessment
Objectives: 
- Finding a Solution to the Problem: The algorithm should reliably find the correct solution within the specified range of inputs.
- Seeking the Optimal Solution: For the same problem, multiple solutions might exist, and we aim to find the most efficient algorithm possible.

2 dimensions:
- Time efficiency: The speed at which an algorithm runs.
- Space efficiency: The size of the memory space occupied by an algorithm.
# 2.2 Iteration and recursion
## 2.2.1   Iteration
### 1.   For loops
```python
for i in range(1, n + 1):
```
```C++
for (int i = 1; i <= n; ++i) {
}
```
### 2.   While loops
```python
while i <= n:
```
```C++
while (i <= n) {
}
```
### 
## 2.2.2   Recursion
2 Phases:
- Calling Phase: The function calls itself with modified parameters.
- Returning Phase: The function returns the result to the previous call.

3 Elements:
- Termination Condition: Determines when to switch from "calling" to "returning."
- Recursive Call: Corresponds to "calling," where the function calls itself, usually with smaller or more simplified parameters.
- Return Result: Corresponds to "returning," where the result of the current recursion level is returned to the previous layer.
```python
def recur(n: int) -> int:
    """Recursion"""
    # Termination condition
    if n == 1:
        return 1
    # Recursive: recursive call
    res = recur(n - 1)
    # Return: return result
    return n + res
```
```C++
/* Recursion */
int recur(int n) {
    // Termination condition
    if (n == 1)
        return 1;
    // Recursive: recursive call
    int res = recur(n - 1);
    // Return: return result
    return n + res;
}
```
- **Iteration**: 
    - Solves problems "from the bottom up." 
    - It starts with the most basic steps, and then repeatedly adds or accumulates these steps until the task is complete.
- **Recursion**: 
    - Solves problems "from the top down." 
    - It breaks down the original problem into smaller sub-problems, each of which has the same form as the original problem. These sub-problems are then further decomposed into even smaller sub-problems, stopping at the base case whose solution is known.
### 2.2.2.1 Call Stack
### 2.2.2.2 Tail Recursion
## 2.2.3 Comparison
||Iteration|Recursion|
|---|---|---|
|Approach|Loop|Function calls itself|
|Time Efficiency|Faster|Slower|
|Memory Usage|Fixed size|Use a substantial amount of stack frame space|
|Suitability|Simple, repetitive|Decompose to smaller sub-problems|

# 2.3  Time complexity
1. **Determining the Running Platform**
- Hardware configuration
- Programming language
- System environment
2. **Evaluating the Run Time for Various Computational Operations**
For instance, an addition operation + might take 1 ns, a multiplication operation * might take 10 ns, a print operation print() might take 5 ns, etc.
3. **Counting All the Computational Operations in the Code**: Summing the execution times of all these operations gives the total run time.

## 2.3.1 Time Growth Trends
- Constant Time: O(1)
- Linear Time: O(n)
- Quadratic Time: O(n^2)

## 2.3.2 Asymptotic upper bound
- $ \exists \, c > 0, n_0 \geq 0: \forall n \geq n_0, T(n) \leq c f(n) $
- then $ f(n) $ is an asymptotic upper bound of $ T(n) $: $O(f(n))$

## 2.3.3 Calculation Method
1. Count the number of operations
